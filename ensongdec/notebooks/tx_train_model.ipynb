{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9cc64f3-7825-40b8-8bb5-09d169705ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import json\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import wandb\n",
    "import datetime\n",
    "from scipy.ndimage import gaussian_filter, gaussian_filter1d\n",
    "\n",
    "def add_to_sys_path(root_dir):\n",
    "    for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "        sys.path.append(dirpath)        \n",
    "root_dir = '/home/jovyan/pablo_tostado/bird_song/enSongDec/'\n",
    "add_to_sys_path(root_dir)\n",
    "\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import warnings\n",
    "from pynwb import NWBHDF5IO\n",
    "from importlib import reload\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from FFNNmodel import FeedforwardNeuralNetwork, ffnn_train, ffnn_evaluate, ffnn_predict\n",
    "from neural_audio_dataset import NeuralAudioDataset\n",
    "import utils.audio_utils as au\n",
    "import utils.encodec_utils as eu\n",
    "import utils.signal_utils as su\n",
    "import utils.train_utils as tu\n",
    "import utils.visualization_utils as vu\n",
    "\n",
    "import songbirdcore.spikefinder.spike_analysis_helper as sh\n",
    "import songbirdcore.spikefinder.filtering_helper as fh\n",
    "\n",
    "# EncoDec\n",
    "from encodec import EncodecModel\n",
    "from encodec.utils import convert_audio\n",
    "\n",
    "# Tim S. noise reduce\n",
    "import noisereduce as nr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d9406bf-17f7-42ee-8bd6-43326737d638",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_dict(d):\n",
    "    \"\"\"\n",
    "    Flatten a nested dictionary into a flat dictionary.\n",
    "    \"\"\"\n",
    "    return {k: v for key, val in d.items() for k, v in (flatten_dict(val).items() if isinstance(val, dict) else [(key, val)])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "baa8b118-f0bd-4811-a97b-a1cb88ca72aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_filepath = \"/home/jovyan/pablo_tostado/bird_song/enSongDec/ensongdec/configs/z_r12r13_21/config_FALCON_thresholds_RA_z_r12r13_21-day1-nwb.json\"\n",
    "\n",
    "# --------- EXPERIMENT CONFIG --------- #\n",
    "    \n",
    "# Extract experiment params from JSON config file\n",
    "with open(config_filepath, 'r') as file:\n",
    "    config = json.load(file)\n",
    "experiment_metadata = flatten_dict(config)\n",
    "\n",
    "# --------- EXTRACT CONFIG INFO --------- #\n",
    "# Directories of interest\n",
    "dataset_dir              = experiment_metadata['dataset_dir']\n",
    "models_checkpoints_dir   = experiment_metadata['models_checkpoints_dir']\n",
    "train_figures_dir        = experiment_metadata['train_figures_dir']\n",
    "# Experiment params\n",
    "dataset_filename         = experiment_metadata['dataset_filename']\n",
    "neural_mode              = experiment_metadata['neural_mode']\n",
    "neural_key               = experiment_metadata['neural_key']\n",
    "bird                     = experiment_metadata['bird']\n",
    "# Config params\n",
    "config_id                = experiment_metadata['config_id']\n",
    "# Data_processing_params\n",
    "neural_history_ms        = experiment_metadata[\"neural_history_ms\"]\n",
    "gaussian_smoothing_sigma = experiment_metadata[\"gaussian_smoothing_sigma\"]\n",
    "# Data_augmentation_params\n",
    "max_temporal_shift_ms    = experiment_metadata[\"max_temporal_shift_ms\"]\n",
    "noise_level              = experiment_metadata[\"noise_level\"]\n",
    "transform_probability    = experiment_metadata[\"transform_probability\"]\n",
    "# Model_params\n",
    "network                  = experiment_metadata['network']\n",
    "hidden_layer_sizes       = experiment_metadata[\"hidden_layer_sizes\"]\n",
    "dropout_prob             = experiment_metadata[\"dropout_prob\"]\n",
    "# Training_params\n",
    "batch_size               = experiment_metadata[\"batch_size\"]\n",
    "learning_rate            = experiment_metadata[\"learning_rate\"]\n",
    "num_epochs               = experiment_metadata[\"num_epochs\"]\n",
    "\n",
    "\n",
    "# Define experiment name to save output files\n",
    "experiment_name = \"_\".join([dataset_filename, network, datetime.datetime.now().strftime('%Y%m%d_%H%M%S')])\n",
    "\n",
    "# Figure List to save to pdf\n",
    "figures_list = []  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "193200f8-cda3-4288-935f-1dbb38f71c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_falcon_nwb(nwb_filepath):\n",
    "\n",
    "    with NWBHDF5IO(nwb_filepath, \"r\") as io:\n",
    "        nwbfile = io.read()\n",
    "    \n",
    "        neural_array = np.array(nwbfile.get_acquisition('tx').data)\n",
    "        spike_times = np.array(nwbfile.get_acquisition('tx').timestamps)\n",
    "        \n",
    "        audio_motifs = np.array(nwbfile.get_acquisition('vocalizations').data)\n",
    "        audio_times = np.array(nwbfile.get_acquisition('vocalizations').timestamps)\n",
    "        \n",
    "        # Trial info\n",
    "        trial_info = (\n",
    "                    nwbfile.trials.to_dataframe()\n",
    "                    .reset_index()\n",
    "        )\n",
    "\n",
    "    print(neural_array.shape, audio_motifs.shape)\n",
    "\n",
    "    # Compute trialized neural array\n",
    "    n_trials = len(trial_info)\n",
    "    n_channels = neural_array.shape[-1]\n",
    "    samples_per_trial = neural_array.shape[0] // n_trials # Number of samples per trial\n",
    "    neural_array = neural_array.reshape(n_trials, samples_per_trial, n_channels) # Trialized spike_matrix\n",
    "    neural_array = neural_array.transpose(0,2,1) # Transpose to [Trials x Channels x Timestamps]\n",
    "    \n",
    "    # Compute trialized audio array\n",
    "    n_trials = len(trial_info)\n",
    "    samples_per_trial = audio_motifs.shape[0] // n_trials # Number of samples per trial\n",
    "    audio_motifs = audio_motifs.reshape(n_trials, samples_per_trial) # Trialized spike_matrix\n",
    "    \n",
    "    print(neural_array.shape, audio_motifs.shape)\n",
    "\n",
    "    fs_neural = 30000\n",
    "    fs_audio = 25000\n",
    "\n",
    "    return neural_array, fs_neural, audio_motifs, fs_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1cf1fd01-858f-4f37-9cbc-5339d163d6a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(783000, 85) (652500,)\n",
      "(29, 85, 27000) (29, 22500)\n"
     ]
    }
   ],
   "source": [
    "# --------- LOAD DATA --------- #\n",
    "\n",
    "data_dir = '/home/jovyan/pablo_tostado/bird_song/finch_Tx_processing/z_r12r13_21/2021-06-27.0727/2021.06.27_nwb_files/'\n",
    "\n",
    "nwb_file_path_calib = data_dir + 'z_r12r13_21_2021.06.27_held_in_calib.nwb'\n",
    "nwb_file_path_eval = data_dir + 'z_r12r13_21_2021.06.27_held_in_eval.nwb'\n",
    "nwb_file_path_minival = data_dir + 'z_r12r13_21_2021.06.27_held_in_minival.nwb'\n",
    "\n",
    "neural_array, fs_neural, audio_motifs, fs_audio = load_falcon_nwb(nwb_file_path_calib)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c4d4e2d-4a3a-4b82-9c23-e5cfef18f961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of neural trials: 900.0 ms, length of audio trials: 900.0 ms. \n"
     ]
    }
   ],
   "source": [
    "# Calculate the duration based on the last dimension of the arrays and their sampling rates\n",
    "trial_length_neural = (neural_array.shape[-1] / fs_neural)*1000\n",
    "trial_length_audio = (audio_motifs.shape[-1] / fs_audio)*1000\n",
    "\n",
    "# Check the durations of the neural/audio data are equal and raise a warning if they aren't\n",
    "print('Length of neural trials: {} ms, length of audio trials: {} ms. '.format(trial_length_neural, trial_length_audio))\n",
    "if trial_length_neural != trial_length_audio:\n",
    "    warnings.warn(\"WARNING: Neural data duration and audio motifs duration are different in this dataset!\")\n",
    "\n",
    "\n",
    "# --------- INSTANTIATE ENCODEC --------- #\n",
    "\n",
    "# Instantiate a pretrained EnCodec model\n",
    "encodec_model = EncodecModel.encodec_model_48khz()\n",
    "# bandwidth = 24kbps for 48kHz model (n_q=16)\n",
    "encodec_model.set_target_bandwidth(24.0)\n",
    "\n",
    "# Embed motifs\n",
    "audio_embeddings, audio_codes, scales = eu.encodec_encode_audio_array_2d(audio_motifs, fs_audio, encodec_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61ab7d41-fa15-421d-9821-c797696e1271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processing neural data as TX\n",
      "History_size: 200, Neural array shape (29, 85, 135), Audio embeddings shape torch.Size([29, 128, 135]).\n",
      "Using 2 bins of neural data history.\n"
     ]
    }
   ],
   "source": [
    "# --------- PROCESS NEURAL --------- #\n",
    "    \n",
    "# Resample neural datato match audio embeddings\n",
    "samples_neural = neural_array.shape[2]\n",
    "samples_embeddings = audio_embeddings.shape[2]\n",
    "history_size = round(samples_neural//samples_embeddings)\n",
    "\n",
    "# Match neural to audio samples (! Different for raw spiketrains vs trajectories)\n",
    "if neural_mode == 'RAW' or neural_mode == 'TX':\n",
    "    print(f'Pre-processing neural data as {neural_mode}')\n",
    "    # Gaussian kernel along the temporal dimension of the spiketrains\n",
    "    neural_array = gaussian_filter1d(neural_array, sigma=gaussian_smoothing_sigma, axis=2) \n",
    "    # Downsample to spikerate at given bin_size\n",
    "    neural_array = sh.downsample_list_3d(neural_array, history_size, mode='sum')  \n",
    "elif neural_mode == 'TRAJECTORIES':\n",
    "    print(f'Pre-processing neural data as {neural_mode}')\n",
    "    # Downsample by interpolation\n",
    "    neural_array = np.array([su.resample_by_interpolation_2d(n, samples_neural, samples_embeddings) for n in neural_array])\n",
    "else:\n",
    "    raise ValueError(\"Neural mode must be 'RAW', 'TX' or 'TRAJECTORIES'\")\n",
    "\n",
    "print(f'History_size: {history_size}, Neural array shape {neural_array.shape}, Audio embeddings shape {audio_embeddings.shape}.')\n",
    "if neural_array.shape[-1] != audio_embeddings.shape[-1]:\n",
    "    warnings.warn(\"WARNING: Neural data length must match audio embeddings length after downsampling!\")\n",
    "\n",
    "bin_length = trial_length_neural / neural_array.shape[2] # ms\n",
    "history_size = int(neural_history_ms // bin_length) # Must be minimum 1\n",
    "print('Using {} bins of neural data history.'.format(history_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d296e572-3d80-470d-92f5-e4cc90c4dea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of the 29 total motifs, the first 17 are used for training. The rest, for testing.\n",
      "Train samples:  3799\n"
     ]
    }
   ],
   "source": [
    "# --------- PREPARE DATALOADERS --------- #\n",
    "    \n",
    "num_motifs = len(neural_array)\n",
    "num_train_examples = int(num_motifs * percent_train)\n",
    "print(f'Out of the {num_motifs} total motifs, the first {num_train_examples} are used for training. The rest, for testing.')\n",
    "\n",
    "# Split the data into train and test sets\n",
    "train_neural = neural_array\n",
    "train_audio = audio_embeddings\n",
    "# test_neural = neural_array[test_idxs]  \n",
    "# test_audio = audio_embeddings[test_idxs]\n",
    "\n",
    "# Create dataset objects\n",
    "max_temporal_shift_bins = int(max_temporal_shift_ms // bin_length) # Temporal jitter for data augmentation\n",
    "\n",
    "train_dataset = NeuralAudioDataset(train_neural, \n",
    "                                   train_audio, \n",
    "                                   history_size, \n",
    "                                   max_temporal_shift=max_temporal_shift_bins,\n",
    "                                   noise_level=noise_level,\n",
    "                                   transform_probability=transform_probability)\n",
    "\n",
    "# test_dataset = NeuralAudioDataset(test_neural, \n",
    "#                                   test_audio, \n",
    "#                                   history_size, \n",
    "#                                   max_temporal_shift=0,\n",
    "#                                   noise_level=0,\n",
    "#                                   transform_probability=0)\n",
    "\n",
    "print('Train samples: ', len(train_dataset))\n",
    "# print('Test samples: ', len(test_dataset))\n",
    "\n",
    "# Prepare data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2b17d71-c3a2-485e-95cc-849278aa4b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input_dim:  170  -  Output dim:  128\n",
      "Layer: layers.0.weight | Size: torch.Size([64, 170]) | Number of Parameters: 10880\n",
      "Layer: layers.0.bias | Size: torch.Size([64]) | Number of Parameters: 64\n",
      "Layer: layers.1.weight | Size: torch.Size([128, 64]) | Number of Parameters: 8192\n",
      "Layer: layers.1.bias | Size: torch.Size([128]) | Number of Parameters: 128\n",
      "Total number of parameters in the model: 19264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mptostado\u001b[0m (\u001b[33mtnel\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/pablo_tostado/bird_song/enSongDec/ensongdec/notebooks/wandb/run-20240601_183757-hx5ibaun</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tnel/z_r12r13_21_TX_ra_all/runs/hx5ibaun' target=\"_blank\">THRESHOLDS_z_r12r13_21_20240524_184625_FFNN_20240601_183711</a></strong> to <a href='https://wandb.ai/tnel/z_r12r13_21_TX_ra_all' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tnel/z_r12r13_21_TX_ra_all' target=\"_blank\">https://wandb.ai/tnel/z_r12r13_21_TX_ra_all</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tnel/z_r12r13_21_TX_ra_all/runs/hx5ibaun' target=\"_blank\">https://wandb.ai/tnel/z_r12r13_21_TX_ra_all/runs/hx5ibaun</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/2000] Training Loss: 1.492528710545612, Training Error: 0.8785115675765927\n",
      "Epoch [100/2000] Training Loss: 1.4735032620550204, Training Error: 0.8717645461819753\n",
      "Epoch [150/2000] Training Loss: 1.4558077599821972, Training Error: 0.8634841787714919\n",
      "Epoch [200/2000] Training Loss: 1.431987902697395, Training Error: 0.8585560602300307\n",
      "Epoch [250/2000] Training Loss: 1.4137994732175554, Training Error: 0.8538477801475204\n",
      "Epoch [300/2000] Training Loss: 1.4086773335432805, Training Error: 0.8513867734860974\n",
      "Epoch [350/2000] Training Loss: 1.3867095924225175, Training Error: 0.8462924266061863\n",
      "Epoch [400/2000] Training Loss: 1.3893477255556763, Training Error: 0.8460453633500749\n",
      "Epoch [450/2000] Training Loss: 1.369128489193796, Training Error: 0.8409458443898112\n",
      "Epoch [500/2000] Training Loss: 1.3777448469851197, Training Error: 0.8434350245139178\n",
      "Epoch [550/2000] Training Loss: 1.381185572688319, Training Error: 0.8427102485624682\n",
      "Epoch [600/2000] Training Loss: 1.3663493394851685, Training Error: 0.8391876025360172\n",
      "Epoch [650/2000] Training Loss: 1.3675059560968095, Training Error: 0.8385872154676614\n",
      "Epoch [700/2000] Training Loss: 1.3568334980171268, Training Error: 0.8366884293676424\n",
      "Epoch [750/2000] Training Loss: 1.3443398655963545, Training Error: 0.8322389431360389\n",
      "Epoch [800/2000] Training Loss: 1.3497911531384252, Training Error: 0.8324049656130686\n",
      "Epoch [850/2000] Training Loss: 1.3597751495217074, Training Error: 0.8357653943430475\n",
      "Epoch [900/2000] Training Loss: 1.3484256142327766, Training Error: 0.8339331019826296\n",
      "Epoch [950/2000] Training Loss: 1.3499137588909693, Training Error: 0.8332720584228259\n",
      "Epoch [1000/2000] Training Loss: 1.3305361005438476, Training Error: 0.8303589901002515\n",
      "Epoch [1050/2000] Training Loss: 1.3433769445459383, Training Error: 0.8309027101813244\n",
      "Epoch [1100/2000] Training Loss: 1.3601798610526974, Training Error: 0.8340903455469789\n",
      "Epoch [1150/2000] Training Loss: 1.3466008976727974, Training Error: 0.8328316958010697\n",
      "Epoch [1200/2000] Training Loss: 1.3486343507005387, Training Error: 0.8332360872701436\n",
      "Epoch [1250/2000] Training Loss: 1.3431715599629057, Training Error: 0.832997047600626\n",
      "Epoch [1300/2000] Training Loss: 1.3401022969173784, Training Error: 0.8317725488117763\n",
      "Epoch [1350/2000] Training Loss: 1.3323603427710653, Training Error: 0.8290735458125588\n",
      "Epoch [1400/2000] Training Loss: 1.3287520869439389, Training Error: 0.8283037613419926\n",
      "Epoch [1450/2000] Training Loss: 1.3372484109982723, Training Error: 0.8292246150369403\n",
      "Epoch [1500/2000] Training Loss: 1.3294888314078837, Training Error: 0.8290394073774835\n",
      "Epoch [1550/2000] Training Loss: 1.337546943115587, Training Error: 0.8312762034039537\n",
      "Epoch [1600/2000] Training Loss: 1.3423865347349344, Training Error: 0.8300987642352321\n",
      "Epoch [1650/2000] Training Loss: 1.3243474599694003, Training Error: 0.8271107698688988\n",
      "Epoch [1700/2000] Training Loss: 1.3318942459691472, Training Error: 0.8297019560797876\n",
      "Epoch [1750/2000] Training Loss: 1.3399462098834896, Training Error: 0.830745416028159\n",
      "Epoch [1800/2000] Training Loss: 1.3309759432528199, Training Error: 0.8299352296260225\n",
      "Epoch [1850/2000] Training Loss: 1.33341097881814, Training Error: 0.8296332234094123\n",
      "Epoch [1900/2000] Training Loss: 1.3318700414745748, Training Error: 0.8302013588552716\n",
      "Epoch [1950/2000] Training Loss: 1.348867321214756, Training Error: 0.8345425534649056\n",
      "Epoch [2000/2000] Training Loss: 1.322588599529587, Training Error: 0.8273476487448236\n",
      "Done training!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "573d4f7042064338aa5e288062162f21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.014 MB of 0.034 MB uploaded\\r'), FloatProgress(value=0.4081485387582915, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_error</td><td>█▆▅▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▂▂▁▂▁</td></tr><tr><td>train_loss</td><td>█▆▅▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▂▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_error</td><td>0.82735</td></tr><tr><td>train_loss</td><td>1.32259</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">THRESHOLDS_z_r12r13_21_20240524_184625_FFNN_20240601_183711</strong> at: <a href='https://wandb.ai/tnel/z_r12r13_21_TX_ra_all/runs/hx5ibaun' target=\"_blank\">https://wandb.ai/tnel/z_r12r13_21_TX_ra_all/runs/hx5ibaun</a><br/> View project at: <a href='https://wandb.ai/tnel/z_r12r13_21_TX_ra_all' target=\"_blank\">https://wandb.ai/tnel/z_r12r13_21_TX_ra_all</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240601_183757-hx5ibaun/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --------- TRAIN MODEL --------- #\n",
    "    \n",
    "# Initialize the neural network and optimizer\n",
    "input, target = next(iter(train_dataset))\n",
    "input_dim = input.shape[0]\n",
    "output_dim = target.shape[0]\n",
    "print('Input_dim: ', input_dim, ' -  Output dim: ', output_dim)\n",
    "\n",
    "# Instantiate model\n",
    "layers = [input_dim] + hidden_layer_sizes + [output_dim]\n",
    "ffnn_model = FeedforwardNeuralNetwork(layers, dropout_prob=dropout_prob)\n",
    "total_params = tu.compute_num_model_params(ffnn_model)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss() \n",
    "optimizer = optim.AdamW(ffnn_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Expand experiment metadata\n",
    "experiment_metadata['config_filepath'] = config_filepath\n",
    "experiment_metadata['experiment_name'] = experiment_name\n",
    "experiment_metadata['layers'] = layers\n",
    "experiment_metadata['total_params'] = total_params\n",
    "\n",
    "# TRACK with WANDB\n",
    "wandb.init(\n",
    "    project = \"_\".join([bird, neural_mode, neural_key]),\n",
    "    name = experiment_name,\n",
    "    config = experiment_metadata # track hyperparameters and run metadata\n",
    ")\n",
    "\n",
    "# Train\n",
    "tot_train_loss, tot_train_err, tot_val_loss, tot_val_err = ffnn_train(ffnn_model, \n",
    "                                                                      train_loader, \n",
    "                                                                      optimizer, \n",
    "                                                                      criterion, \n",
    "                                                                      num_epochs, \n",
    "                                                                      val_dataloader=None)\n",
    "print('Done training!')\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba94612-a4d2-44e5-858e-81f9f8dadd4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b440086-fc81-4476-9fae-8ba921edff4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e0cfd7-6784-4fbb-8c04-2284777d2041",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d46be5f-fd21-44b1-841f-85a2dbfecbfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ce2b88-8f1d-4a71-94cc-85af2fb64071",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_dict(d):\n",
    "    \"\"\"\n",
    "    Flatten a nested dictionary into a flat dictionary.\n",
    "    \"\"\"\n",
    "    return {k: v for key, val in d.items() for k, v in (flatten_dict(val).items() if isinstance(val, dict) else [(key, val)])}\n",
    "\n",
    "    \n",
    "def main(config_filepath, override_dict=None):\n",
    "\n",
    "    # --------- EXPERIMENT CONFIG --------- #\n",
    "    \n",
    "    # Extract experiment params from JSON config file\n",
    "    with open(config_filepath, 'r') as file:\n",
    "        config = json.load(file)\n",
    "    experiment_metadata = flatten_dict(config)\n",
    "\n",
    "    # GRID SEARCH: Override param\n",
    "    if override_dict:\n",
    "        for k, v in override_dict.items():\n",
    "            override_experiment_metadata = experiment_metadata.copy()\n",
    "            if k in experiment_metadata:\n",
    "                for k_val in v:\n",
    "                    override_experiment_metadata[k] = k_val\n",
    "                    print(f'Updated {k} to {k_val}')\n",
    "                    run_experiment(override_experiment_metadata, config_filepath)\n",
    "            else:\n",
    "                print(f'{k} not found in experiment_metadata. Skipping override.')\n",
    "    else:\n",
    "        run_experiment(experiment_metadata, config_filepath)\n",
    "\n",
    "def run_experiment(experiment_metadata, config_filepath):\n",
    "\n",
    "    # --------- EXTRACT CONFIG INFO --------- #\n",
    "    # Directories of interest\n",
    "    dataset_dir              = experiment_metadata['dataset_dir']\n",
    "    models_checkpoints_dir   = experiment_metadata['models_checkpoints_dir']\n",
    "    train_figures_dir        = experiment_metadata['train_figures_dir']\n",
    "    # Experiment params\n",
    "    dataset_filename         = experiment_metadata['dataset_filename']\n",
    "    neural_mode              = experiment_metadata['neural_mode']\n",
    "    neural_key               = experiment_metadata['neural_key']\n",
    "    bird                     = experiment_metadata['bird']\n",
    "    # Config params\n",
    "    config_id                = experiment_metadata['config_id']\n",
    "    # Data_processing_params\n",
    "    neural_history_ms        = experiment_metadata[\"neural_history_ms\"]\n",
    "    gaussian_smoothing_sigma = experiment_metadata[\"gaussian_smoothing_sigma\"]\n",
    "    # Data_augmentation_params\n",
    "    max_temporal_shift_ms    = experiment_metadata[\"max_temporal_shift_ms\"]\n",
    "    noise_level              = experiment_metadata[\"noise_level\"]\n",
    "    transform_probability    = experiment_metadata[\"transform_probability\"]\n",
    "    # Model_params\n",
    "    network                  = experiment_metadata['network']\n",
    "    hidden_layer_sizes       = experiment_metadata[\"hidden_layer_sizes\"]\n",
    "    dropout_prob             = experiment_metadata[\"dropout_prob\"]\n",
    "    # Training_params\n",
    "    percent_train            = experiment_metadata[\"percent_train\"]\n",
    "    percent_test             = experiment_metadata[\"percent_test\"]\n",
    "    batch_size               = experiment_metadata[\"batch_size\"]\n",
    "    learning_rate            = experiment_metadata[\"learning_rate\"]\n",
    "    num_epochs               = experiment_metadata[\"num_epochs\"]\n",
    "\n",
    "    # Define experiment name to save output files\n",
    "    experiment_name = \"_\".join([dataset_filename, network, datetime.datetime.now().strftime('%Y%m%d_%H%M%S')])\n",
    "\n",
    "    # Figure List to save to pdf\n",
    "    figures_list = []  \n",
    "    \n",
    "    # --------- LOAD DATA --------- #\n",
    "    \n",
    "    # Open the file in binary read mode ('rb') and load the content using pickle\n",
    "    with open(dataset_dir + dataset_filename + '.pkl', 'rb') as file:\n",
    "        data_dict = pkl.load(file)\n",
    "    \n",
    "    # Extarct data from dataset\n",
    "    if neural_mode == 'RAW' or neural_mode == 'TX':\n",
    "        neural_array = data_dict['neural_dict'][neural_key]\n",
    "    elif neural_mode == 'TRAJECTORIES':\n",
    "        latent_mode = experiment_metadata['latent_mode']\n",
    "        dimensionality = experiment_metadata['dimensionality']\n",
    "        neural_array = data_dict['neural_dict'][neural_key][latent_mode][neural_key+'_dim'+str(dimensionality)]['trajectories']\n",
    "    audio_motifs = data_dict['audio_motifs']\n",
    "    fs_audio = data_dict['fs_audio']\n",
    "    fs_neural = data_dict['fs_neural']\n",
    "    \n",
    "    # Calculate the duration based on the last dimension of the arrays and their sampling rates\n",
    "    trial_length_neural = (neural_array.shape[-1] / fs_neural)*1000\n",
    "    trial_length_audio = (audio_motifs.shape[-1] / fs_audio)*1000\n",
    "    \n",
    "    # Check the durations of the neural/audio data are equal and raise a warning if they aren't\n",
    "    print('Length of neural trials: {} ms, length of audio trials: {} ms. '.format(trial_length_neural, trial_length_audio))\n",
    "    if trial_length_neural != trial_length_audio:\n",
    "        warnings.warn(\"WARNING: Neural data duration and audio motifs duration are different in this dataset!\")\n",
    "\n",
    "    # Plot and save figures for raw neural_array and audio_motifs\n",
    "    title = 'Raw Neural Traces'\n",
    "    figures_list.append(vu.visualize_neural(neural_array, title=title, neural_channel=10, offset=1))\n",
    "    title = 'Raw Audio Motifs'\n",
    "    figures_list.append(vu.visualize_audio(audio_motifs, title=title, offset=40000))\n",
    "\n",
    "    \n",
    "    # --------- PROCESS AUDIO --------- #\n",
    "    \n",
    "    b, a = fh.load_filter_coefficients_matlab(\n",
    "        '/home/jovyan/pablo_tostado/repos/songbirdcore/songbirdcore/filters/butter_bp_250Hz-8000hz_order4_sr25000.mat')\n",
    "    audio_motifs = fh.noncausal_filter_2d(audio_motifs, b=b, a=a)\n",
    "    \n",
    "    # Reduce noise\n",
    "    for m in range(len(audio_motifs)):\n",
    "        audio_motifs[m] = nr.reduce_noise(audio_motifs[m], sr=fs_audio)\n",
    "    \n",
    "    \n",
    "    # --------- INSTANTIATE ENCODEC --------- #\n",
    "    \n",
    "    # Instantiate a pretrained EnCodec model\n",
    "    encodec_model = EncodecModel.encodec_model_48khz()\n",
    "    # bandwidth = 24kbps for 48kHz model (n_q=16)\n",
    "    encodec_model.set_target_bandwidth(24.0)\n",
    "    \n",
    "    # Embed motifs\n",
    "    audio_embeddings, audio_codes, scales = eu.encodec_encode_audio_array_2d(audio_motifs, fs_audio, encodec_model)\n",
    "    \n",
    "    \n",
    "    # --------- PROCESS NEURAL --------- #\n",
    "    \n",
    "    # Resample neural datato match audio embeddings\n",
    "    samples_neural = neural_array.shape[2]\n",
    "    samples_embeddings = audio_embeddings.shape[2]\n",
    "    history_size = samples_neural//samples_embeddings\n",
    "    \n",
    "    # Match neural to audio samples (! Different for raw spiketrains vs trajectories)\n",
    "    if neural_mode == 'RAW' or neural_mode == 'TX':\n",
    "        print(f'Pre-processing neural data as {neural_mode}')\n",
    "        # Gaussian kernel along the temporal dimension of the spiketrains\n",
    "        neural_array = gaussian_filter1d(neural_array, sigma=gaussian_smoothing_sigma, axis=2) \n",
    "        # Downsample to spikerate at given bin_size\n",
    "        neural_array = sh.downsample_list_3d(neural_array, history_size, mode='sum')  \n",
    "    elif neural_mode == 'TRAJECTORIES':\n",
    "        print(f'Pre-processing neural data as {neural_mode}')\n",
    "        # Downsample by interpolation\n",
    "        neural_array = np.array([su.resample_by_interpolation_2d(n, samples_neural, samples_embeddings) for n in neural_array])\n",
    "    else:\n",
    "        raise ValueError(\"Neural mode must be 'RAW', 'TX' or 'TRAJECTORIES'\")\n",
    "    \n",
    "    bin_length = trial_length_neural / neural_array.shape[2] # ms\n",
    "    history_size = int(neural_history_ms // bin_length) # Must be minimum 1\n",
    "    print('Using {} bins of neural data history.'.format(history_size))\n",
    "\n",
    "    # Plot and save figures for resampled neural_array and audio_embeddings\n",
    "    title = 'Resampled Neural Traces {}'.format(neural_array.shape)\n",
    "    figures_list.append(vu.visualize_neural(neural_array, title=title, neural_channel=10, offset=1))\n",
    "    title = 'Audio Embeddings {}'.format(audio_embeddings.shape)\n",
    "    figures_list.append(vu.visualize_audio_embeddings(audio_embeddings, title=title, embedding_dim=0, offset=10))\n",
    "\n",
    "    \n",
    "    # --------- PREPARE DATALOADERS --------- #\n",
    "    \n",
    "    num_motifs = len(neural_array)\n",
    "    num_train_examples = int(num_motifs * percent_train)\n",
    "    print(f'Out of the {num_motifs} total motifs, the first {num_train_examples} are used for training. The rest, for testing.')\n",
    "    \n",
    "    # Split the data into train and test sets\n",
    "    train_idxs = list(range(0, num_train_examples))\n",
    "    test_idxs = list(range(num_train_examples, num_motifs))\n",
    "    train_neural = neural_array[train_idxs]  \n",
    "    train_audio = audio_embeddings[train_idxs]\n",
    "    test_neural = neural_array[test_idxs]  \n",
    "    test_audio = audio_embeddings[test_idxs]\n",
    "    \n",
    "    # Create dataset objects\n",
    "    max_temporal_shift_bins = int(max_temporal_shift_ms // bin_length) # Temporal jitter for data augmentation\n",
    "    \n",
    "    train_dataset = NeuralAudioDataset(train_neural, \n",
    "                                       train_audio, \n",
    "                                       history_size, \n",
    "                                       max_temporal_shift=max_temporal_shift_bins,\n",
    "                                       noise_level=noise_level,\n",
    "                                       transform_probability=transform_probability)\n",
    "    \n",
    "    test_dataset = NeuralAudioDataset(test_neural, \n",
    "                                      test_audio, \n",
    "                                      history_size, \n",
    "                                      max_temporal_shift=0,\n",
    "                                      noise_level=0,\n",
    "                                      transform_probability=0)\n",
    "    \n",
    "    print('Train samples: ', len(train_dataset))\n",
    "    print('Test samples: ', len(test_dataset))\n",
    "    \n",
    "    # Prepare data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    \n",
    "    # --------- TRAIN MODEL --------- #\n",
    "    \n",
    "    # Initialize the neural network and optimizer\n",
    "    input, target = next(iter(train_dataset))\n",
    "    input_dim = input.shape[0]\n",
    "    output_dim = target.shape[0]\n",
    "    print('Input_dim: ', input_dim, ' -  Output dim: ', output_dim)\n",
    "    \n",
    "    # Instantiate model\n",
    "    layers = [input_dim] + hidden_layer_sizes + [output_dim]\n",
    "    ffnn_model = FeedforwardNeuralNetwork(layers, dropout_prob=dropout_prob)\n",
    "    total_params = tu.compute_num_model_params(ffnn_model)\n",
    "    \n",
    "    # Loss function and optimizer\n",
    "    criterion = nn.MSELoss() \n",
    "    optimizer = optim.AdamW(ffnn_model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Expand experiment metadata\n",
    "    experiment_metadata['config_filepath'] = config_filepath\n",
    "    experiment_metadata['experiment_name'] = experiment_name\n",
    "    experiment_metadata['layers'] = layers\n",
    "    experiment_metadata['total_params'] = total_params\n",
    "    experiment_metadata['train_idxs'] = train_idxs\n",
    "    experiment_metadata['test_idxs'] = test_idxs\n",
    "    \n",
    "    # TRACK with WANDB\n",
    "    wandb.init(\n",
    "        project = \"_\".join([bird, neural_mode, neural_key]),\n",
    "        name = experiment_name, k\n",
    "        config = experiment_metadata # track hyperparameters and run metadata\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    tot_train_loss, tot_train_err, tot_val_loss, tot_val_err = ffnn_train(ffnn_model, \n",
    "                                                                          train_loader, \n",
    "                                                                          optimizer, \n",
    "                                                                          criterion, \n",
    "                                                                          num_epochs, \n",
    "                                                                          val_dataloader=test_loader)\n",
    "    print('Done training!')\n",
    "    wandb.finish()\n",
    "\n",
    "    # Plot and save figures for loss and error\n",
    "    title = 'Loss'\n",
    "    figures_list.append(vu.visualize_loss_error(tot_train_loss, tot_val_loss, title=title))\n",
    "    title = 'Embeddings Reconstruction Error'\n",
    "    figures_list.append(vu.visualize_loss_error(tot_train_err, tot_val_err, title=title))\n",
    "\n",
    "    # Initialize PdfPages for saving figures to PDF\n",
    "    try:\n",
    "        print('figures ', len(figures_list))\n",
    "        figures_filename = os.path.join(train_figures_dir, f'{experiment_name}_figures.pdf') \n",
    "        pdf_pages = PdfPages(figures_filename)\n",
    "        for fig in figures_list:\n",
    "            pdf_pages.savefig(fig)\n",
    "        pdf_pages.close()\n",
    "    except Exception as e:\n",
    "        if pdf_pages is not None:\n",
    "            pdf_pages.close()  \n",
    "\n",
    "    # --------- SAVE MODEL --------- #\n",
    "    if not os.path.exists(models_checkpoints_dir):\n",
    "        os.makedirs(models_checkpoints_dir)\n",
    "\n",
    "    # Expand experiment metadata\n",
    "    experiment_metadata['tot_train_loss'] = tot_train_loss\n",
    "    experiment_metadata['tot_train_err'] = tot_train_err\n",
    "    experiment_metadata['tot_val_loss'] = tot_val_loss\n",
    "    experiment_metadata['tot_val_err'] = tot_val_err\n",
    "    \n",
    "    tu.save_model(models_checkpoints_dir, experiment_name, ffnn_model, optimizer, experiment_metadata)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Run the bird song decoding experiment.\")\n",
    "    parser.add_argument(\"--config_filepath\", type=str, required=True, help=\"Path to JSON config file of the experiment.\")\n",
    "    parser.add_argument(\"--override_dict\", type=str, required=False, help=\"Dictionary params to override in the config file one at a time, e.g. {'learning_rate': [0.02, 0.07], 'dropout_prob': [0.25]}\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Parse the override_dict JSON string into a dictionary\n",
    "    override_dict = None\n",
    "    if args.override_dict:\n",
    "        override_dict = json.loads(args.override_dict)\n",
    "    \n",
    "    main(args.config_filepath, override_dict=override_dict)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
